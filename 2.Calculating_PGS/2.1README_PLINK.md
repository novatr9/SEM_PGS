# Background
This is a markdown to compute PGS using `plink` - while `plink` is not a dedicated PGS software, it can perform every required steps of the C+T approach. 
This multi-step analysis is a good way to learn the processes involved in computing PGS, which are typically performed automatically by PGS softwares like LDpred2.

## 2.1.0 Required Data

In the previous sections, the following files were generated:

|File Name | Description|
|:-:|:-:|
|**trait.QC.gz**| The post-QCed summary statistic |
|**target.QC.bed**| The genotype file after performing some basic filtering |
|**target.QC.bim**| This file contains the SNPs that passed the basic filtering |
|**target.QC.fam**| This file contains the samples that passed the basic filtering |
|**target.trait**| This file contains the phenotype of the samples |
|**target.cov**| This file contains the covariates of the samples |


## 2.1.1 Update Effect Size
When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PGS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PGS can be computed using summation instead (which can be back-transformed afterwards). 
We can obtain the transformed summary statistics with `R`:


```R
dat <- read.table(gzfile("trait.QC.gz"), header=T)
dat$BETA <- log(dat$OR)
write.table(dat, "trait.QC.Transformed", quote=F, row.names=F)
q() # exit R
```




 **/!\ Warning** 
|:-:|
Due to rounding of values, using `awk` to log transform OR can lead to less accurate results. Therefore, it is recommended to perform the transformation in `R` or allow the PGS software to perform the transformation directly.

## 2.1.2 Clumping
Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, 
makes identifying the contribution from causal independent genetic variants extremely challenging. 
One way of approximately capturing the right level of causal signal is to perform clumping, 
which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining 
the SNPs most associated with the phenotype under study. 
Clumping can be performed using the following command in `plink`: 

```bash
plink \
    --bfile target.QC \
    --clump-p1 1 \
    --clump-r2 0.1 \
    --clump-kb 250 \
    --clump trait.QC.Transformed \
    --clump-snp-field SNP \
    --clump-field P \
    --out target
```

Each of the new parameters corresponds to the following

| Parameter | Value | Description|
|:-:|:-:|:-|
| clump-p1 | 1 | P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping|
| clump-r2 | 0.1 | SNPs having $r^2$ higher than 0.1 with the index SNPs will be removed |
| clump-kb | 250 | SNPs within 250k of the index SNP are considered for clumping|
| clump | trait.QC.Transformed | Base data (summary statistic) file containing the P-value information|
| clump-snp-field | SNP | Specifies that the column `SNP` contains the SNP IDs |
| clump-field | P | Specifies that the column `P` contains the P-value information |

A more detailed description of the clumping process can be found [here](https://www.cog-genomics.org/plink/1.9/postproc#clump)

 **/!\ Note** 
|:-:|
The $r^2$ values computed by `--clump` are based on maximum likelihood haplotype frequency estimates


This will generate **target.clumped**, containing the index SNPs after clumping is performed.
We can extract the index SNP ID by performing the following command:

```awk
awk 'NR!=1{print $3}' target.clumped >  target.valid.snp
```

> `$3` because the third column contains the SNP ID


 **/!\ Note** 
|:-:|
If your target data are small (e.g. N < 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample.

## 2.1.3 Generate PGS
`plink` provides a convenient function `--score` and `--q-score-range` for calculating polygenic scores.

We will need three files:

1. The base data file: **trait.QC.Transformed**
2. A file containing SNP IDs and their corresponding P-values (`$3` because SNP ID is located in the third column; `$8` because the P-value is located in the eighth column)

```awk
awk '{print $3,$8}' trait.QC.Transformed > SNP.pvalue
```

3. A file containing the different P-value thresholds for inclusion of SNPs in the PGS. Here calculate PGS corresponding to a few thresholds for illustration purposes:

```bash
echo "0.001 0 0.001" > range_list 
echo "0.05 0 0.05" >> range_list
echo "0.1 0 0.1" >> range_list
echo "0.2 0 0.2" >> range_list
echo "0.3 0 0.3" >> range_list
echo "0.4 0 0.4" >> range_list
echo "0.5 0 0.5" >> range_list
```
The format of the **range_list** file should be as follows:

|Name of Threshold|Lower bound| Upper Bound|
|:-:|:-:|:-:|

 **/!\ Note** 
|:-:|
The threshold boundaries are inclusive. For example, for the `0.05` threshold, we include all SNPs with P-value from `0` to `0.05`, **including** any SNPs with P-value equal to `0.05`.

We can then calculate the PGS with the following `plink` command:

```bash
plink \
    --bfile target.QC \
    --score trait.QC.Transformed 3 4 12 header \
    --q-score-range range_list SNP.pvalue \
    --extract target.valid.snp \
    --out target
```
The meaning of the new parameters are as follows:

| Paramter | Value | Description|
|:-:|:-:|:-|
|score|trait.QC.Transformed 3 4 12 header| We read from the **trait.QC.Transformed** file, assuming that the `3`st column is the SNP ID; `4`th column is the effective allele information; the `12`th column is the effect size estimate; and that the file contains a `header`|
|q-score-range| range_list SNP.pvalue| We want to calculate PGS based on the thresholds defined in **range_list**, where the threshold values (P-values) were stored in **SNP.pvalue**|

The above command and range_list will generate 7 files:

1. target.0.5.profile
2. target.0.4.profile
3. target.0.3.profile
4. target.0.2.profile
5. target.0.1.profile
6. target.0.05.profile
7. target.0.001.profile

 **/!\ Note** 
|:-:|
The default formula for PGS calculation in PLINK is:  

$`
PGS_j =\frac{ \sum_i^NS_i*G_{ij}}{P*M_j}
    `$

where the effect size of SNP $i$ is $S_i$;  the number of effect alleles observed in sample $j$ is $G_{ij}$; the ploidy of the sample is $P$ (is generally 2 for humans); the total number of SNPs included in the PGS is $N$; and the number of non-missing SNPs observed in sample $j$ is $M_j$. If the sample has a missing genotype for SNP $i$, then the population minor allele frequency multiplied by the ploidy ($MAF_i*P$) is used instead of $G_{ij}$.

## 2.1.4 Accounting for Population Stratification

Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PGS analysis to account for population stratification.

Again, we can calculate the PCs using `plink`: 

```bash
# First, we need to perform prunning
plink \
    --bfile target.QC \
    --indep-pairwise 200 50 0.25 \
    --out target
# Then we calculate the first 6 PCs
plink \
    --bfile target.QC \
    --extract target.prune.in \
    --pca 6 \
    --out target
```

 **/!\ Note** 
|:-:|
One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. [LDSC](https://github.com/bulik/ldsc) analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. 

Here the PCs have been stored in the **target.eigenvec** file and can be used as covariates in the regression model to account for population stratification.

 **/!\ Important** 
|:-:|
If the base and target samples are collected from different worldwide populations then the results from the PGS analysis may be biased (see Section 3.4 of our paper).


## 2.1.5 Finding the "best-fit" PGS
The P-value threshold that provides the "best-fit" PGS under the C+T method is usually unknown. 
To approximate the "best-fit" PGS, we can perform a regression between PGS calculated at a range of P-value thresholds and then select the PGS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). 
This can be achieved using `R` as follows:


```R 
p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5)
# Read in the phenotype file 
phenotype <- read.table("target.trait", header=T)
# Read in the PCs
pcs <- read.table("target.eigenvec", header=F)
# The default output from plink does not include a header
# To make things simple, we will add the appropriate headers
# (1:6 because there are 6 PCs)
colnames(pcs) <- c("FID", "IID", paste0("PC",1:6)) 
# Read in the covariates (here, it is sex)
covariate <- read.table("target.cov", header=T)
# Now merge the files
pheno <- merge(merge(phenotype, covariate, by=c("FID", "IID")), pcs, by=c("FID","IID"))
# We can then calculate the null model (model with PGS) using a linear regression 
# (as trait is quantitative)
null.model <- lm(trait~., data=pheno[,!colnames(pheno)%in%c("FID","IID")])
# And the R2 of the null model is 
null.r2 <- summary(null.model)$r.squared
PGS.result <- NULL
for(i in p.threshold){
  # Go through each p-value threshold
  PGS <- read.table(paste0("target.",i,".profile"), header=T)
  # Merge the PGS with the phenotype matrix
  # We only want the FID, IID and PGS from the PGS file, therefore we only select the 
  # relevant columns
  pheno.PGS <- merge(pheno, PGS[,c("FID","IID", "SCORE")], by=c("FID", "IID"))
  # Now perform a linear regression on trait with PGS and the covariates
  # ignoring the FID and IID from our model
  model <- lm(trait~., data=pheno.PGS[,!colnames(pheno.PGS)%in%c("FID","IID")])
  # model R2 is obtained as 
  model.r2 <- summary(model)$r.squared
  # R2 of PGS is simply calculated as the model R2 minus the null R2
  PGS.r2 <- model.r2-null.r2
  # We can also obtain the coeffcient and p-value of association of PGS as follow
  PGS.coef <- summary(model)$coeff["SCORE",]
  PGS.beta <- as.numeric(PGS.coef[1])
  PGS.se <- as.numeric(PGS.coef[2])
  PGS.p <- as.numeric(PGS.coef[4])
  # We can then store the results
  PGS.result <- rbind(PGS.result, data.frame(Threshold=i, R2=PGS.r2, P=PGS.p, BETA=PGS.beta,SE=PGS.se))
}
# Best result is:
PGS.result[which.max(PGS.result$R2),]
q() # exit R
```

Here's a quicker but less readable/user friendly:

```R
p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5)
phenotype <- read.table("target.trait", header=T)
pcs <- read.table("target.eigenvec", header=F)
colnames(pcs) <- c("FID", "IID", paste0("PC",1:6)) 
covariate <- read.table("target.cov", header=T)
pheno <- merge(merge(phenotype, covariate, by=c("FID", "IID")), pcs, by=c("FID","IID"))
null.r2 <- summary(lm(trait~., data=pheno[,!colnames(pheno)%in%c("FID","IID")]))$r.squared
PGS.result <- NULL
for(i in p.threshold){
  pheno.PGS <- merge(pheno, 
    read.table(paste0("target.",i,".profile"), header=T)[,c("FID","IID", "SCORE")],
    by=c("FID", "IID"))
  model <- summary(lm(trait~., data=pheno.PGS[,!colnames(pheno.PGS)%in%c("FID","IID")]))
  model.r2 <- model$r.squared
  PGS.r2 <- model.r2-null.r2
  PGS.coef <- model$coeff["SCORE",]
  PGS.result <- rbind(PGS.result, 
    data.frame(Threshold=i, R2=PGS.r2, 
      P=as.numeric(PGS.coef[4]), 
      BETA=as.numeric(PGS.coef[1]),
      SE=as.numeric(PGS.coef[2])))
}
print(PGS.result[which.max(PGS.result$R2),])
q() # exit R
```







